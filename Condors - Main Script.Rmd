---
title: "Landscape Modelling Project - Condors - Main Script"
author: "Justine DeGroote, Tanja Falasca, Manuel Weber"
date: '2023-03-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Landscape Modelling Project - Condors

## Main Script

##### Students: Justine DeGroote, Tanja Falasca, Manuel Weber

##### Supervisor: Monika K. Goralczyk

## Organization

Weekly meetings on Tuesdays at 4:15 PM on the green floor in CHN or in F77 (no meeting on the 11th of April).

Summarize and email questions to Monika before the meetings.

| Date        | Progress Milestone                                                                          | Suggested Focus                                                     |
|--------------|----------------------------------|------------------------|
| 03.04-07.04 | Species occurrence data downloaded and cleaned                                              | Data retrieval and cleaning                                         |
| 10.04-14.05 | Environmental data prepared for model fitting                                               | Environmental exploration                                           |
| 17.04-21.04 | 1.Initial set of predictors selected 2.Created 1st set of PAs (e.g.: random)                | Environmental predictors ; Pseudoabsence creation (PAs)             |
| 24.04-28.04 | Buffered, target-group, environmentally stratified PAs                                      | Pseudoabsence creation (PAs)                                        |
| 01.05-05.05 | Models fitted and evaluated (at least 2 algorithms)                                         | Model fitting & evaluation                                          |
| 08.05-12.05 | Evidence of exploring different model parameters, selected ones with best fit and accuracy. | Generated predictor response curves Model fitting & model selection |
| 15.05-19.05 | Creation of ranges (polygons for mammals)                                                   | Alternative approach to SDM                                         |
| 22.05-26.05 | Overlap of distributions and protected areas                                                | Implications for conservation                                       |
| 29.05       | Presentation                                                                                |                                                                     |

: Preliminary schedule

#### SDM resources

Zurell, D. et al. (2020) "A standard protocol for reporting species distribution models," Ecography, 43(9), pp. 1261--1277. Available at: <https://doi.org/10.1111/ecog.04960>

ENM2020 course (table 1 contains description of tutorials) <https://journals.ku.edu/jbi/article/view/15016/15152>

For cleaning occurrence data (from the ENM 2020 course)

-   'Tools for biodiversity data cleaning' -- R packages for data cleaning

-   'Occurrence data cleaning I (simple consistency checks)' -- main manual checks

Some information about habitat and diet of the 3 focal species

-   <https://avianreport.com/andean-condor-range-and-habitat/>

-   [https://nationalzoo.si.edu/animals/andean-bear#:\~:text=Andean%20bears%20live%20in%20a](https://nationalzoo.si.edu/animals/andean-bear#:~:text=Andean%20bears%20live%20in%20a,forest%20to%20thorny%20dry%20forest.){.uri}

-   [forest%20to%20thorny%20dry%20forest.](https://nationalzoo.si.edu/animals/andean-bear#:~:text=Andean%20bears%20live%20in%20a,forest%20to%20thorny%20dry%20forest.){.uri} <https://animalia.bio/south-american-cougar>

## Chunk 1: Species Occurrence Data Download (Manuel)

This chunk acquires occurrence data of the Andean Condor, the Puma and the Andean Bear from GBIF. The data is limited to geo-referenced data points from South America, recorded between 2000 and 2023. The raw data is saved in the repository in the folder "Occurrence data" under "gbi\_*species abbreviation*.csv".

```{r}
# Data acquisition
# Load the rgbif R package
# library(rgbif)
# 
# # Download occurrences for the three species
# ## Vultur gryphus
# gbi_vg <- occ_search(scientificName = "Vultur gryphus", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_vg$data, "gbi_vg.csv")
# 
# ## Puma concolor
# gbi_pc <- occ_search(scientificName = "Puma concolor", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_pc$data, "gbi_pc.csv")
# 
# ## Tremarctos ornatus
# gbi_to <- occ_search(scientificName = "Tremarctos ornatus", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_to$data, "gbi_to.csv")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 6/4; 13:22)
############################################################################################

```

## Chunk 2: Species Occurrence Data Cleaning, Thinning and Visualizing (Manuel)

The following chunk cleans the data ( among others: removal of duplicates, zeros, points referenced to seas or research institutions). It proceeds by extracting essential columns (coordinates, uncertainties and occurrence status) and cropping the data to northern South America, by using the mean latitude of the condor occurrence data as cut-off line. The uncertainty of the cleaned data was then quantified as follows:

| Uncertainty    | Fraction of the retained data points |
|----------------|--------------------------------------|
| No Uncertainty | 91 %                                 |
| \>100 km       | 0.6 % of 9 %                         |
| \>10 km        | 63.6 % of 9 %                        |
| \>1 km         | 71.5 % of 9 %                        |

Although most of the referenced data had uncertainties superior to 10 km, we decide to proceed with a resolution of 1 km. The occurrence data is then re-projected to the area-conservative coordinate reference system Albers Equal Area ESRI:102033. The data is stored in the repository in the folder "Occurrence data" under "*species*\_final.csv".

The data is thinned so that individual data points are spaced by at least 1 km. The obtained data-sets contain only coordinates and occurrence stati and are saved in the repository in the folder "Occurrence data" under "*species*\_presences_1km.csv". Finally, the data is visualized in a basic plot:

![Basic distribution maps of the cleaned data](distribution%20maps%20basic.jpeg)

```{r}
# # Cleaning, projecting and visualizing data
# # Loading packages
# lib_vect <- c("terra", "ranger", "ecospat", "dismo", "rworldmap", "CoordinateCleaner", "dplyr")
# sapply(lib_vect,require,character.only=TRUE)
# 
# # Read in data
# condor <- read.csv("Occurence data/gbi_vg.csv")
# puma <- read.csv("Occurence data/gbi_pc.csv")
# bear <- read.csv("Occurence data/gbi_to.csv")
# 
# # Cleaning data
# # The sepecies infomration can be used to help identify duplicates. We can also identify other criteria to test for.
# coordinate_flags_condor <- clean_coordinates(x=condor, lon="decimalLongitude", lat="decimalLatitude", countries="countryCode", species="scientificName", tests= c("centroids","gbif","institutions","duplicates","seas","zeros"), verbose=T)
# 
# coordinate_flags_puma <- clean_coordinates(x=puma, lon="decimalLongitude", lat="decimalLatitude", countries="countryCode", species="scientificName", tests= c("centroids","gbif","institutions","duplicates","seas","zeros"), verbose=T)
# 
# coordinate_flags_bear <- clean_coordinates(x=bear, lon="decimalLongitude", lat="decimalLatitude", countries="countryCode", species="scientificName", tests= c("centroids","gbif","institutions","duplicates","seas","zeros"), verbose=T)
# 
# # Add wheteher or not the points were flagged to the datafame
# condor$flags <- as.factor(coordinate_flags_condor$.summary)
# puma$flags <- as.factor(coordinate_flags_puma$.summary)
# bear$flags <- as.factor(coordinate_flags_bear$.summary)
# 
# # Remove the problematic points
# condor <- subset(condor, condor$flags=="TRUE")
# puma <- subset(puma, puma$flags=="TRUE")
# bear <- subset(bear, bear$flags=="TRUE")
# 
# # Extract essential columns
# gbi_vg_crd <- condor[,c("decimalLongitude", "decimalLatitude", 
#                         "coordinateUncertaintyInMeters", "occurrenceStatus")]
# gbi_vg_crd$occurrenceStatus <- as.factor(gbi_vg_crd$occurrenceStatus)
# gbi_pc_crd <- puma[,c("decimalLongitude", "decimalLatitude", 
#                       "coordinateUncertaintyInMeters", "occurrenceStatus")]
# gbi_pc_crd$occurrenceStatus <- as.factor(gbi_pc_crd$occurrenceStatus)
# gbi_to_crd <- bear[,c("decimalLongitude", "decimalLatitude", 
#                       "coordinateUncertaintyInMeters", "occurrenceStatus")]
# gbi_to_crd$occurrenceStatus <- as.factor(gbi_to_crd$occurrenceStatus)
# 
# # Crop data to northern South America
# gbi_vg_crd <- filter(gbi_vg_crd, decimalLatitude >= mean(condor$decimalLatitude))
# nrow(gbi_vg_crd)
# ## 5490 observations left
# gbi_pc_crd <- filter(gbi_pc_crd, decimalLatitude >= mean(condor$decimalLatitude))
# nrow(gbi_pc_crd)
# ## 1340 observations left
# gbi_to_crd <- filter(gbi_to_crd, decimalLatitude >= mean(condor$decimalLatitude))
# nrow(gbi_to_crd)
# ## 2152 observations left
# 
# # Fraction of observations with no information on coordinate precision
# sna_vg <- length(which(is.na(gbi_vg_crd$coordinateUncertaintyInMeters)))
# sna_pc <- length(which(is.na(gbi_pc_crd$coordinateUncertaintyInMeters)))
# sna_to <- length(which(is.na(gbi_to_crd$coordinateUncertaintyInMeters)))
# 
# round((sna_vg+sna_pc+sna_to)/(nrow(gbi_vg_crd)+nrow(gbi_pc_crd)+nrow(gbi_to_crd))*100, digits = 2)
# ## 91% of the data doesn't contains no information at all about coordinate precision
# 
# # Fraction of imprecise observations
# tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=100000)
# tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=100000)
# tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=100000)
# 
# round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
# ## 0.6% of the observations are less precise than 100 km
# 
# tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=10000)
# tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=10000)
# tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=10000)
# 
# round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
# ## 63.6 % of the observations are less precise than 10 km
# 
# tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=1000)
# tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=1000)
# tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=1000)
# 
# round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
# ## 71.5% of the observations are less precise than 1 km
# 
# library(rworldmap)
# # Transforming the occurence data to Albers Equal Area ESRI:102033, using the DEM as template
# dem <- rast("Rasters/DEM.tif")
# 
# coordinates(gbi_vg_crd) <- c("decimalLongitude", "decimalLatitude")
# coordinates(gbi_pc_crd) <- c("decimalLongitude", "decimalLatitude")
# coordinates(gbi_to_crd) <- c("decimalLongitude", "decimalLatitude")
# 
# raster::crs(gbi_vg_crd) <-  'EPSG:4326'
# raster::crs(gbi_pc_crd) <-  'EPSG:4326'
# raster::crs(gbi_to_crd) <-  'EPSG:4326' ### we first need to set the crs before being able to transform it
# 
# gbi_vg_crd <- spTransform(gbi_vg_crd, CRS(terra::crs(dem)))
# gbi_pc_crd <- spTransform(gbi_pc_crd, CRS(terra::crs(dem)))
# gbi_to_crd <- spTransform(gbi_to_crd, CRS(terra::crs(dem)))
# 
# gbi_vg_crd <- as.data.frame(gbi_vg_crd)
# gbi_pc_crd <- as.data.frame(gbi_pc_crd)
# gbi_to_crd <- as.data.frame(gbi_to_crd)
# 
# # Saving the preprocessed data
# write.csv(gbi_vg_crd, "Occurence data/condor_final.csv")
# write.csv(gbi_pc_crd, "Occurence data/puma_final.csv")
# write.csv(gbi_to_crd, "Occurence data/bear_final.csv")
# 
# # Visualizing the data
# ## Create spatial vectors with the projection of interest (esri:102033)
# 
# ve_vg = vect(gbi_vg_crd,geom=c("decimalLongitude","decimalLatitude"),
#              crs=crs(dem))
# ve_pc = vect(gbi_pc_crd,geom=c("decimalLongitude","decimalLatitude"),
#              crs=crs(dem))
# ve_to = vect(gbi_to_crd,geom=c("decimalLongitude","decimalLatitude"),
#              crs=crs(dem))
#
# data(coastsCoarse)
# raster::crs(coastsCoarse) <- "EPSG:4326"
# coastsCoarse <- spTransform(coastsCoarse, CRS(terra::crs(dem)))
#
# {par(mfrow = c(1,3))
# plot(ve_vg, main = expression(paste(italic('Vultur gryphus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
# plot(coastsCoarse, add = T, col = "grey")
# plot(ve_pc, main = expression(paste(italic('Puma concolor'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
# plot(coastsCoarse, add = T, col = "grey")
# plot(ve_to, main = expression(paste(italic('Tremarctos ornatus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
# plot(coastsCoarse, add = T, col = "grey")}

# Finally we thin the data, so that only one presence is kept per raster cell of 1x1 km (the DEM serves as template)

# condor_raster <- rasterize(ve_vg, dem, fun = "length")
# puma_raster <- rasterize(ve_pc, dem, fun = "length")
# bear_raster <- rasterize(ve_to, dem, fun = "length")

# condor_pres <- crds(condor_raster)
# puma_pres <- crds(puma_raster)
# bear_pres <- crds(bear_raster)

# condor_pres <- as.data.frame(condor_pres)
# condor_pres$Presence <- rep(1, nrow(condor_pres))
# puma_pres <- as.data.frame(puma_pres)
# puma_pres$Presence <- rep(1, nrow(puma_pres))
# bear_pres <- as.data.frame(bear_pres)
# bear_pres$Presence <- rep(1, nrow(bear_pres))

# write.csv(condor_pres, "Occurence data/condor_presences_1km.csv")
# write.csv(puma_pres, "Occurence data/puma_presences_1km.csv")
# write.csv(bear_pres, "Occurence data/bear_presences_1km.csv")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 20/4; 13:45)
############################################################################################

```

## Chunk 3: Generating Pseudo-absences (Manuel)

```{r}
# Generating Pseudo-absences

#1# Random
# Loading packages
lib_vect <- c("terra", "ranger", "ecospat", "dismo", "rworldmap", "CoordinateCleaner", "dplyr")
sapply(lib_vect,require,character.only=TRUE)

# We use the DEM as template and environmental stratification a bit later. First we need to crop the DEM to the study area.
dem <- rast("Rasters/DEM.tif")
library(sf)
area <- read_sf("Study area shapefile/study area shapefile.shp")
area <- area[1]
dem_cropped <- crop(dem, area)
dem_cropped <- mask(dem_cropped, area)
plot(dem_cropped)

# Sample randomly 10k pseudo-absences
rnd <- spatSample(dem_cropped, 10000, method = "random", na.rm = TRUE,
                  as.df = TRUE, xy = TRUE)
# Remove NAs
rnd  <- na.omit(rnd)

# Add absence information
rnd$Presence <- 0

# Visualize
plot(dem_cropped)
points(rnd)
write.csv(rnd, "Pseudo absences/pseudo absences random.csv")
```

## Chunk 4: Predictor Acquisition and Preprocessing

Goal: selecting, cropping, reprojecting, and aligning rasters.

Rasters to consider:

-   Bioclimatic (obtain from CHELSA <https://chelsa-climate.org/bioclim/>)

-   Tree cover

-   Tree height

-   DEM (later calculate aspect and slope in R

-   Population density

-   Distance to water

-   Landscape openness

Decide on trade-off between grain size and extent: For now we use 1x1 km (original resolution of DEM)

Splitting data into months, fitting months separately (is one habitat more important for nesting?)

Environmental or spatial stratification of pseudo-absences

We start with random pseudo-absences

We choose to use an area-conservative projection: Albers equal area conic projection (epsg.io)

We will crop all layers to the study area (shapefile located in the folder study area shapefile)

```{r}
# Generating slope and aspect from DEM:
# library(terra)
# (dem <- rast("Rasters/DEM.tif"))
# slope <- terrain(x = dem, v = "slope")
# aspect <- terrain(x = dem, v = "aspect")
# 
# writeRaster(slope, "Rasters/slope.tif")
# writeRaster(aspect, "Rasters/aspect.tif")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 13/4; 17:47)
############################################################################################

#insert Rasters

##population density
pop_dens <- raster("C:/Users/justi/OneDrive/Studium/Umweltnaturwissenschaften/Landscape Modelling/Condor/download polybox/gpw_v4_population_density_rev11_2020_30_sec.tif")

### Climatic predictors
## Temperature
# Mean annual air temperature
bio1 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio1_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')
#Annual range of air temperature
bio7 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio7_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')

## Precipitation
# Annual Precipitation amount
bio12 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio12_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')
# Precipitation amount of the wettest month
bio13 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio13_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')
# Precipitation amount of the driest month
bio14 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio14_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')

## Tree cover
tree_cover <- rast("C:\Users\tanja\OneDrive\Dokumente\UZH FS_23\Landscape Modelling\Project\API_AG.LND.FRST.ZS_DS2_en_csv_v2_5358376\API_AG.LND.FRST.ZS_DS2_en_csv_v2_5358376.csv")

#convert to Spatraster
slope <- as(slope, "SpatRaster")
aspect <- as(aspect, "SpatRaster")
pop_dens <- as(pop_dens, "SpatRaster")
dem <- as(dem, "SpatRaster")

# Reproject the *bio* raster so they have the same spatial properties as the *dem* raster
bio1 <- project(bio1, dem)
bio7 <- project(bio7, dem)
bio12 <- project(bio12, dem)
bio13 <- project(bio13, dem)
bio14 <- project(bio14, dem)
pop_dens <- project(pop_dens, dem)

## Create one predictor layer
# Check if the resolution, origin and coordinate reference system match among the different layers with the *compareGeom* function
compareGeom(dem, slope, aspect, pop_dens, bio1, bio7, bio12, bio13, bio14)

#store raster
writeRaster(pop_dens, "Rasters/pop_dens.tif")
writeRaster(bio1, "Rasters/bio1.tif")
writeRaster(bio7, "Rasters/bio7.tif")
writeRaster(bio12, "Rasters/bio12.tif")
writeRaster(bio13, "Rasters/bio13.tif")
writeRaster(bio14, "Rasters/bio14.tif")

# Stacking of the layers
predictors <- c(dem, slope, aspect, pop_dens, bio1, bio7, bio12, bio13, bio14)
names(predictors) <- c('dem', 'slope', 'aspect', 'pop_dens', 'bio1', 'bio7', 'bio12', 'bio13', 'bio14')
predictors
```

Extracting environmental conditions for presences and absences:

    df_pres_aa <- cbind(extract(environmental_predictors,crd_aa_thin), crd_aa_thin)

Code for reprojection:

dem_reprojected \<- terra::project(x = dem, y = "ESRI:102033", method = "near")
