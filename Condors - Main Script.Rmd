---
title: "Landscape Modelling Project - Condors - Main Script"
author: "Justine DeGroote, Tanja Falasca, Manuel Weber"
date: '2023-03-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Landscape Modelling Project - Condors

## Main Script

Students: Justine DeGroote, Tanja Falasca, Manuel Weber

Supervisor: Monika K. Goralczyk

## Organization

Weekly meetings on Tuesdays at 4:15 PM on the green floor in CHN or in F77 (no meeting on the 11th of April).

Summarize and email questions to Monika before the meetings.

| Date        | Progress Milestone                                                                          | Suggested Focus                                                     |
|---------------|---------------------------------|------------------------|
| 03.04-07.04 | Species occurrence data downloaded and cleaned                                              | Data retrieval and cleaning                                         |
| 10.04-14.05 | Environmental data prepared for model fitting                                               | Environmental exploration                                           |
| 17.04-21.04 | 1.Initial set of predictors selected 2.Created 1st set of PAs (e.g.: random)                | Environmental predictors ; Pseudoabsence creation (PAs)             |
| 24.04-28.04 | Buffered, target-group, environmentally stratified PAs                                      | Pseudoabsence creation (PAs)                                        |
| 01.05-05.05 | Models fitted and evaluated (at least 2 algorithms)                                         | Model fitting & evaluation                                          |
| 08.05-12.05 | Evidence of exploring different model parameters, selected ones with best fit and accuracy. | Generated predictor response curves Model fitting & model selection |
| 15.05-19.05 | Creation of ranges (polygons for mammals)                                                   | Alternative approach to SDM                                         |
| 22.05-26.05 | Overlap of distributions and protected areas                                                | Implications for conservation                                       |
| 29.05       | Presentation                                                                                |                                                                     |

: Preliminary schedule

#### SDM resources

Zurell, D. et al. (2020) "A standard protocol for reporting species distribution models," Ecography, 43(9), pp. 1261--1277. Available at: <https://doi.org/10.1111/ecog.04960>

ENM2020 course (table 1 contains description of tutorials) <https://journals.ku.edu/jbi/article/view/15016/15152>

For cleaning occurrence data (from the ENM 2020 course)

-   'Tools for biodiversity data cleaning' -- R packages for data cleaning

-   'Occurrence data cleaning I (simple consistency checks)' -- main manual checks

Some information about habitat and diet of the 3 focal species

-   <https://avianreport.com/andean-condor-range-and-habitat/>

-   [https://nationalzoo.si.edu/animals/andean-bear#:\~:text=Andean%20bears%20live%20in%20a](https://nationalzoo.si.edu/animals/andean-bear#:~:text=Andean%20bears%20live%20in%20a,forest%20to%20thorny%20dry%20forest.){.uri}

-   [forest%20to%20thorny%20dry%20forest.](https://nationalzoo.si.edu/animals/andean-bear#:~:text=Andean%20bears%20live%20in%20a,forest%20to%20thorny%20dry%20forest.){.uri} <https://animalia.bio/south-american-cougar>

## Chunk 1: Species Occurrence Data Acquisition (Manuel)

This chunk acquires occurrence data of the Andean Condor, the Puma and the Andean Bear from GBIF. The data is limited to geo-referenced data points from South America, recorded between 2000 and 2023. The raw data is saved in the repository in the folder "Occurrence data" under "gbi\_*species abbreviation*.csv".

```{r}
# Data acquisition
# Load the rgbif R package
# library(rgbif)
# 
# # Download occurrences for the three species
# ## Vultur gryphus
# gbi_vg <- occ_search(scientificName = "Vultur gryphus", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_vg$data, "gbi_vg.csv")
# 
# ## Puma concolor
# gbi_pc <- occ_search(scientificName = "Puma concolor", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_pc$data, "gbi_pc.csv")
# 
# ## Tremarctos ornatus
# gbi_to <- occ_search(scientificName = "Tremarctos ornatus", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_to$data, "gbi_to.csv")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 6/4; 13:22)
############################################################################################

```

## Chunk 2: Species Occurrence Data Cleaning and Thinning (Manuel)

The following chunk cleans the data ( among others: removal of duplicates, zeros, points referenced to seas or research institutions). It proceeds by extracting essential columns (coordinates, uncertainties and occurrence status) and cropping the data to northern South America, by using the mean latitude of the condor occurrence data as cut-off line. The uncertainty of the cleaned data was then quantified as follows:

| Uncertainty    | Fraction of the retained data points |
|----------------|--------------------------------------|
| No Uncertainty | 91 %                                 |
| \>100 km       | 0.6 % of 9 %                         |
| \>10 km        | 63.6 % of 9 %                        |
| \>1 km         | 71.5 % of 9 %                        |

Although most of the referenced data had uncertainties superior to 10 km, we decide to proceed with a resolution of 1 km. The occurrence data is then re-projected to the area-conservative coordinate reference system Albers Equal Area ESRI:102033. The data is stored in the repository in the folder "Occurrence data" under "*species*\_final.csv".

The data is thinned so that individual data points are spaced by at least 1 km. The obtained data-sets contain only coordinates and occurrence stati and are saved in the repository in the folder "Occurrence data" under "*species*\_presences_1km.csv". Finally, the data is visualized in a basic plot:

![](Visualizations/distribution%20maps%20basic.jpeg)

```{r}
# # Cleaning, projecting and visualizing data
# # Loading packages
# lib_vect <- c("terra", "ranger", "ecospat", "dismo", "rworldmap", "CoordinateCleaner", "dplyr")
# sapply(lib_vect,require,character.only=TRUE)
# 
# # Read in data
# condor <- read.csv("Occurence data/gbi_vg.csv")
# puma <- read.csv("Occurence data/gbi_pc.csv")
# bear <- read.csv("Occurence data/gbi_to.csv")
# 
# # Cleaning data
# # The sepecies infomration can be used to help identify duplicates. We can also identify other criteria to test for.
# coordinate_flags_condor <- clean_coordinates(x=condor, lon="decimalLongitude", lat="decimalLatitude", countries="countryCode", species="scientificName", tests= c("centroids","gbif","institutions","duplicates","seas","zeros"), verbose=T)
# 
# coordinate_flags_puma <- clean_coordinates(x=puma, lon="decimalLongitude", lat="decimalLatitude", countries="countryCode", species="scientificName", tests= c("centroids","gbif","institutions","duplicates","seas","zeros"), verbose=T)
# 
# coordinate_flags_bear <- clean_coordinates(x=bear, lon="decimalLongitude", lat="decimalLatitude", countries="countryCode", species="scientificName", tests= c("centroids","gbif","institutions","duplicates","seas","zeros"), verbose=T)
# 
# # Add wheteher or not the points were flagged to the datafame
# condor$flags <- as.factor(coordinate_flags_condor$.summary)
# puma$flags <- as.factor(coordinate_flags_puma$.summary)
# bear$flags <- as.factor(coordinate_flags_bear$.summary)
# 
# # Remove the problematic points
# condor <- subset(condor, condor$flags=="TRUE")
# puma <- subset(puma, puma$flags=="TRUE")
# bear <- subset(bear, bear$flags=="TRUE")
# 
# # Extract essential columns
# gbi_vg_crd <- condor[,c("decimalLongitude", "decimalLatitude", 
#                         "coordinateUncertaintyInMeters", "occurrenceStatus")]
# gbi_vg_crd$occurrenceStatus <- as.factor(gbi_vg_crd$occurrenceStatus)
# gbi_pc_crd <- puma[,c("decimalLongitude", "decimalLatitude", 
#                       "coordinateUncertaintyInMeters", "occurrenceStatus")]
# gbi_pc_crd$occurrenceStatus <- as.factor(gbi_pc_crd$occurrenceStatus)
# gbi_to_crd <- bear[,c("decimalLongitude", "decimalLatitude", 
#                       "coordinateUncertaintyInMeters", "occurrenceStatus")]
# gbi_to_crd$occurrenceStatus <- as.factor(gbi_to_crd$occurrenceStatus)
# 
# # Crop data to northern South America
# gbi_vg_crd <- filter(gbi_vg_crd, decimalLatitude >= mean(condor$decimalLatitude))
# nrow(gbi_vg_crd)
# ## 5490 observations left
# gbi_pc_crd <- filter(gbi_pc_crd, decimalLatitude >= mean(condor$decimalLatitude))
# nrow(gbi_pc_crd)
# ## 1340 observations left
# gbi_to_crd <- filter(gbi_to_crd, decimalLatitude >= mean(condor$decimalLatitude))
# nrow(gbi_to_crd)
# ## 2152 observations left
# 
# # Fraction of observations with no information on coordinate precision
# sna_vg <- length(which(is.na(gbi_vg_crd$coordinateUncertaintyInMeters)))
# sna_pc <- length(which(is.na(gbi_pc_crd$coordinateUncertaintyInMeters)))
# sna_to <- length(which(is.na(gbi_to_crd$coordinateUncertaintyInMeters)))
# 
# round((sna_vg+sna_pc+sna_to)/(nrow(gbi_vg_crd)+nrow(gbi_pc_crd)+nrow(gbi_to_crd))*100, digits = 2)
# ## 91% of the data doesn't contains no information at all about coordinate precision
# 
# # Fraction of imprecise observations
# tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=100000)
# tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=100000)
# tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=100000)
# 
# round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
# ## 0.6% of the observations are less precise than 100 km
# 
# tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=10000)
# tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=10000)
# tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=10000)
# 
# round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
# ## 63.6 % of the observations are less precise than 10 km
# 
# tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=1000)
# tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=1000)
# tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=1000)
# 
# round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
# ## 71.5% of the observations are less precise than 1 km
# 
# library(rworldmap)
# # Transforming the occurence data to Albers Equal Area ESRI:102033, using the DEM as template
# dem <- rast("Rasters/DEM.tif")
# 
# coordinates(gbi_vg_crd) <- c("decimalLongitude", "decimalLatitude")
# coordinates(gbi_pc_crd) <- c("decimalLongitude", "decimalLatitude")
# coordinates(gbi_to_crd) <- c("decimalLongitude", "decimalLatitude")
# 
# raster::crs(gbi_vg_crd) <-  'EPSG:4326'
# raster::crs(gbi_pc_crd) <-  'EPSG:4326'
# raster::crs(gbi_to_crd) <-  'EPSG:4326' ### we first need to set the crs before being able to transform it
# 
# gbi_vg_crd <- spTransform(gbi_vg_crd, CRS(terra::crs(dem)))
# gbi_pc_crd <- spTransform(gbi_pc_crd, CRS(terra::crs(dem)))
# gbi_to_crd <- spTransform(gbi_to_crd, CRS(terra::crs(dem)))
# 
# gbi_vg_crd <- as.data.frame(gbi_vg_crd)
# gbi_pc_crd <- as.data.frame(gbi_pc_crd)
# gbi_to_crd <- as.data.frame(gbi_to_crd)
# 
# # Saving the preprocessed data
# write.csv(gbi_vg_crd, "Occurence data/condor_final.csv")
# write.csv(gbi_pc_crd, "Occurence data/puma_final.csv")
# write.csv(gbi_to_crd, "Occurence data/bear_final.csv")
# 
# # Visualizing the data
# ## Create spatial vectors with the projection of interest (esri:102033)
# 
# ve_vg = vect(gbi_vg_crd,geom=c("decimalLongitude","decimalLatitude"),
#              crs=crs(dem))
# ve_pc = vect(gbi_pc_crd,geom=c("decimalLongitude","decimalLatitude"),
#              crs=crs(dem))
# ve_to = vect(gbi_to_crd,geom=c("decimalLongitude","decimalLatitude"),
#              crs=crs(dem))
#
# data(coastsCoarse)
# raster::crs(coastsCoarse) <- "EPSG:4326"
# coastsCoarse <- spTransform(coastsCoarse, CRS(terra::crs(dem)))
#
# {par(mfrow = c(1,3))
# plot(ve_vg, main = expression(paste(italic('Vultur gryphus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
# plot(coastsCoarse, add = T, col = "grey")
# plot(ve_pc, main = expression(paste(italic('Puma concolor'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
# plot(coastsCoarse, add = T, col = "grey")
# plot(ve_to, main = expression(paste(italic('Tremarctos ornatus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
# plot(coastsCoarse, add = T, col = "grey")}

# Finally we thin the data, so that only one presence is kept per raster cell of 1x1 km (the DEM serves as template)

# condor_raster <- rasterize(ve_vg, dem, fun = "length")
# puma_raster <- rasterize(ve_pc, dem, fun = "length")
# bear_raster <- rasterize(ve_to, dem, fun = "length")

# condor_pres <- crds(condor_raster)
# puma_pres <- crds(puma_raster)
# bear_pres <- crds(bear_raster)

# condor_pres <- as.data.frame(condor_pres)
# condor_pres$Presence <- rep(1, nrow(condor_pres))
# puma_pres <- as.data.frame(puma_pres)
# puma_pres$Presence <- rep(1, nrow(puma_pres))
# bear_pres <- as.data.frame(bear_pres)
# bear_pres$Presence <- rep(1, nrow(bear_pres))

# write.csv(condor_pres, "Occurence data/condor_presences_1km.csv")
# write.csv(puma_pres, "Occurence data/puma_presences_1km.csv")
# write.csv(bear_pres, "Occurence data/bear_presences_1km.csv")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 20/4; 13:45)
############################################################################################

```

## Chunk 3: Generating Pseudo-absences (Manuel)

The following chunk produces three sets of pseudo-absences. The data is stored in the "Pseudo absences" folder under "pseudo absences *type species*.csv".

First, a set of 10 000 spatially random pseudo absences is generated and visualized as follows:

![](Visualizations/pseudo%20absences%20random.jpeg)

Next, 10 000 geographically stratified pseudo-absences are generated. This entails to aggregate the 1x1km resolution DEM to a 10x10km resolution to keep computing times reasonable. Furthermore, the stratification of the topography was implemented in classes of 100 meters.

![](Visualizations/pseudo%20absences%20stratified.jpeg)

Finally, three sets of 10 000 geographically weighed pseudo-absences were generated, one for each species. These pseudo-absences are aggregated around presences.

![](Visualizations/pseudo%20absences%20geographically%20weighed.jpeg)

```{r}
# # Generating Pseudo-absences
# 
# # Random ######################################################################
# # Loading packages
# lib_vect <- c("terra", "ranger", "ecospat", "dismo", "rworldmap", "CoordinateCleaner", "dplyr")
# sapply(lib_vect,require,character.only=TRUE)
# 
# # We use the DEM as template and environmental stratification a bit later. First we need to crop the DEM to the study area.
# dem <- rast("Rasters/DEM.tif")
# library(sf)
# area <- read_sf("Study area shapefile/study area shapefile.shp")
# area <- area[1]
# dem_cropped <- crop(dem, area)
# dem_cropped <- mask(dem_cropped, area)
# plot(dem_cropped)
# 
# # Sample randomly 10k pseudo-absences
# rnd <- spatSample(dem_cropped, 10000, method = "random", na.rm = TRUE,
#                   as.df = TRUE, xy = TRUE)
# # Remove NAs
# rnd  <- na.omit(rnd)
# 
# # Add absence information
# rnd$Presence <- 0
# 
# # Visualize
# {plot(dem_cropped, main = "Pseudo Absences: Random (DEM)")
# points(rnd, cex = 0.1)}
# write.csv(rnd, "Pseudo absences/pseudo absences random.csv")
# 
# # Environmentally stratified #############################################################
# 
# ## We'll use the DEM as stratifier
# 
# library(terra)
# dem <- rast("Rasters/DEM.tif")
# library(sf)
# area <- read_sf("Study area shapefile/study area shapefile.shp")
# area <- area[1]
# dem_cropped <- crop(dem, area)
# dem_cropped <- mask(dem_cropped, area)
# plot(dem_cropped)
# 
# 
# 
# dem_df <- as.data.frame(dem, xy = T, na.rm = T)
# dem_df$elevation <- round(dem_df$elevation, digits = -3)
# length(unique(dem_df$elevation))
# dem_rounded <- rast(dem_df)
# dem_rounded_cropped <- crop(dem_rounded, area)
# dem_rounded_cropped <- mask(dem_rounded_cropped, area)
# plot(dem_rounded_cropped)
# strat <- spatSample(dem_rounded_cropped, 10000, method = "stratified", na.rm = TRUE,
#                     as.df = TRUE, xy = TRUE, replace = T)
# 
# # Remove NAs
# strat  <- na.omit(strat)
# 
# # Add absence information
# strat$Presence <- 0
# 
# 
# {plot(dem_rounded_cropped, main = "Pseudo Absences: Stratified (DEM)")
#   points(strat, cex = 0.1)}
# write.csv(rnd, "Pseudo absences/pseudo absences stratified elevation.csv")
# 
# 
# 
# # Geographic weighting ##################################################################
# 
# # Load dismo package
# suppressPackageStartupMessages(library(dismo))
# 
# # Reading in occurrence data
# condor <- read.csv("Occurrence data/condor_presences_1km.csv")
# puma <- read.csv("Occurrence data/puma_presences_1km.csv")
# bear <- read.csv("Occurrence data/bear_presences_1km.csv")
# 
# 
# # First, sample a regular grid of absence points (as many points as presence observations)
# reg_sam_condor <- spatSample(dem_cropped, nrow(condor), xy = TRUE, 
#                          method = "regular", na.rm = TRUE, as.df = TRUE)
# reg_sam_puma <- spatSample(dem_cropped, nrow(puma), xy = TRUE,
#                          method = "regular", na.rm = TRUE, as.df = TRUE)
# reg_sam_bear <- spatSample(dem_cropped, nrow(bear), xy = TRUE,
#                            method = "regular", na.rm = TRUE, as.df = TRUE)
# 
# points(reg_sam_bear)
# # Fit IDW models
# mod_idw_condor <- geoIDW(p = as.data.frame(condor[,c("x","y")]),
#                      a = reg_sam_condor[,c("x","y")])
# 
# mod_idw_puma <- geoIDW(p = as.data.frame(puma[,c("x","y")]),
#                      a = reg_sam_puma[,c("x","y")])
# 
# mod_idw_bear <- geoIDW(p = as.data.frame(bear[,c("x","y")]),
#                      a = reg_sam_bear[,c("x","y")])
# 
# 
# # predicting the values requires a raster layer (not a spatraster layer)
# dem <- raster::raster("Rasters/DEM.tif")
# dem_cropped_raster <- crop(dem, area)
# dem_cropped_raster <- raster::aggregate(dem_cropped_raster, fact = 10)
# dem_cropped_raster <- mask(dem_cropped_raster, area)
# plot(dem_cropped_raster)
# 
# # We change the resolution from 1 to 10 km, otherwise it takes too long
# 
# # Predict test run
# prd_idw_condor <- raster::predict(object = dem_cropped_raster, model = mod_idw_condor, mask = TRUE)
# prd_idw_puma <- raster::predict(object = dem_cropped_raster, model = mod_idw_puma, mask = TRUE)
# prd_idw_bear <- raster::predict(object = dem_cropped_raster, model = mod_idw_bear, mask = TRUE)
# 
# # Define lists to store replicates of idw predictons
# idw_condor <- idw_puma <- idw_bear <- list()
# # Repeat ten times
# for(i in 1:5){
#   
#   # Refit idw model on jittered absences
#   mod_idw_condor <- geoIDW(p = as.data.frame(condor[,c("x","y")]),
#                        a = as.data.frame(apply(reg_sam_condor[,c("x","y")],2, jitter, 
#                                                factor = 3)))
#   
#   mod_idw_puma <- geoIDW(p = as.data.frame(puma[,c("x","y")]),
#                        a = as.data.frame(apply(reg_sam_puma[,c("x","y")],2, jitter, 
#                                                factor = 3)))
#   
#   mod_idw_bear <- geoIDW(p = as.data.frame(bear[,c("x","y")]),
#                        a = as.data.frame(apply(reg_sam_bear[,c("x","y")],2, jitter, 
#                                                factor = 3)))
#   # Predict
#   idw_condor[[i]] <- predict(dem_cropped_raster, mod_idw_condor, mask = TRUE)
#   idw_puma[[i]] <- predict(dem_cropped_raster, mod_idw_puma, mask = TRUE)
#   idw_bear[[i]] <- predict(dem_cropped_raster, mod_idw_bear, mask = TRUE)
# }
# 
# # Average across the ten replicates
# prdj_idw_condor <- mean(stack(idw_condor))
# prdj_idw_puma <- mean(stack(idw_puma))
# prdj_idw_bear <- mean(stack(idw_bear))
# 
# # Convert rasters to data.frames
# df_idw_condor <- as.data.frame(as(prdj_idw_condor, "SpatialPixelsDataFrame"))
# df_idw_puma <- as.data.frame(as(prdj_idw_puma, "SpatialPixelsDataFrame"))
# df_idw_bear <- as.data.frame(as(prdj_idw_bear, "SpatialPixelsDataFrame"))
# 
# # Detach raster package (so it does not interfere with terra)
# detach("package:dismo")
# detach("package:raster")
# 
# # Sample according to derived probability
# smp_condor <- sample(1:nrow(df_idw_condor), size = 10000, prob = df_idw_condor$layer)
# smp_puma <- sample(1:nrow(df_idw_puma), size = 10000, prob = df_idw_puma$layer)
# smp_bear <- sample(1:nrow(df_idw_bear), size = 10000, prob = df_idw_bear$layer)
# 
# # Prepare coordinates for extraction
# crd_geo_condor <- as.matrix(df_idw_condor[smp_condor, c("x", "y")])
# crd_geo_puma <- as.matrix(df_idw_puma[smp_puma, c("x", "y")])
# crd_geo_bear <- as.matrix(df_idw_bear[smp_bear, c("x", "y")])
# 
# # Extract environmental conditons at sampling locations
# geo_condor <- cbind(extract(dem_cropped, crd_geo_condor), crd_geo_condor)
# geo_puma <- cbind(extract(dem_cropped, crd_geo_puma), crd_geo_puma)
# geo_bear <- cbind(extract(dem_cropped, crd_geo_bear), crd_geo_bear)
# 
# # Remove NAs
# geo_condor <- na.omit(geo_condor)
# geo_puma <- na.omit(geo_puma)
# geo_bear <- na.omit(geo_bear)
# 
# # Add presence/absence information
# geo_condor$Presence <- 0
# geo_puma$Presence <- 0
# geo_bear$Presence <- 0
# 
# {par(mfrow = c(2,3))
# plot(dem_cropped, main = "Pseudo Absences Condor: Geographically weighed (DEM)")
# points(geo_condor[,c("x","y")], cex = 0.1)
# plot(dem_cropped, main = "Pseudo Absences Puma: Geographically weighed (DEM)")
# points(geo_puma[,c("x","y")], cex = 0.1)
# plot(dem_cropped, main = "Pseudo Absences Andean Bear: Geographically weighed (DEM)")
# points(geo_bear[,c("x","y")], cex = 0.1)}
# 
# 
# write.csv(geo_condor, "Pseudo absences/pseudo absences geographically weighed condor.csv")
# write.csv(geo_puma, "Pseudo absences/pseudo absences geographically weighed puma.csv")
# write.csv(geo_bear, "Pseudo absences/pseudo absences geographically weighed bear.csv")
# 
# # Finally, we check if the five generated files all have 10 000 entries
# cols <- numeric(length(list.files("Pseudo absences/")))
# for(i in 1:length(list.files("Pseudo absences/"))){
#   data <- read.csv(paste0("Pseudo absences/", list.files("Pseudo absences/")[i], sep = ""))
#   cols[i] <- nrow(data)
# }
# cols

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 23/4; 16:10)
############################################################################################

```

## Chunk 4: Predictor Acquisition and Pre-processing (Justine & Tanja)

Goal: selecting, cropping, reprojecting, and aligning rasters.

Rasters to consider:

-   Bioclimatic (obtain from CHELSA <https://chelsa-climate.org/bioclim/>)

-   Tree cover

-   Tree height

-   DEM (later calculate aspect and slope in R

-   Population density

-   Distance to water (generated from water bodies (lakes + rivers))

-   Landscape openness

Decide on trade-off between grain size and extent: For now we use 1x1 km (original resolution of DEM)

Splitting data into months, fitting months separately (is one habitat more important for nesting?)

Environmental or spatial stratification of pseudo-absences

We start with random pseudo-absences

We choose to use an area-conservative projection: Albers equal area conic projection (epsg.io)

We will crop all layers to the study area (shapefile located in the folder study area shapefile)

We will test for autocorrelations between the predictors (threshold in the lecture at 0.7).

```{r}
# Generating slope and aspect from DEM:
# library(terra)
# (dem <- rast("Rasters/DEM.tif"))
# slope <- terrain(x = dem, v = "slope")
# aspect <- terrain(x = dem, v = "aspect")
# 
# writeRaster(slope, "Rasters/slope.tif")
# writeRaster(aspect, "Rasters/aspect.tif")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 13/4; 17:47)
############################################################################################

#insert Rasters

##population density
pop_dens <- raster("C:/Users/justi/OneDrive/Studium/Umweltnaturwissenschaften/Landscape Modelling/Condor/download polybox/gpw_v4_population_density_rev11_2020_30_sec.tif")

library('Rcpp')
library('terra')
library('ggsignif')
library('raster')

### DEM
dem <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\DEM_aea.tif')
plot(dem, axes=F)


#### Predictors
### Temperature
## Mean annual air temperature (bio1)
#temp_mean <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio1_1981-2010_V.2.1.tif')
## Annual range of air temperature (bio7)
#temp_range <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio7_1981-2010_V.2.1.tif')

### Precipitation
## Annual Precipitation amount (bio12)
#prec_annual <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio12_1981-2010_V.2.1.tif')
## Precipitation amount of the wettest month (bio13)
#prec_wettest_month <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio13_1981-2010_V.2.1.tif')
## Precipitation amount of the driest month (bio14)
#prec_driest_month <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio14_1981-2010_V.2.1.tif')

## Tree cover
tree_cover <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\tree_cover_2000_aea.tif')
plot(tree_cover, axes=F)

## Canopy height
canopy_height <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\canopy_height_aea.tif')
plot(canopy_height, axes=F)


#### Create one predictor layer
### Check if the resolution, origin and coordinate reference system match among the different layers with the #*compareGeom* function
compareGeom(dem, temp_mean, temp_range, prec_annual, prec_wettest_month, prec_driest_month, tree_cover, canopy_height)

sapply(list(dem=dem, temp_mean=temp_mean, temp_range=temp_range, prec_annual=prec_annual, prec_wettest_month=prec_wettest_month, prec_dries_month=prec_driest_month, tree_cover=tree_cover, canopy_height=canopy_height),
       FUN=function(x){c(origin=origin(x), res=res(x), ext=ext(x)[1:4])})

# Reproject the *bio* raster so they have the same spatial properties as the *dem* raster (*tree_cover* and *canopy_height* already have)
temp_mean <- project(temp_mean, dem)
temp_range <- project(temp_range, dem)
prec_annual <- project(prec_annual, dem)
prec_wettest_month <- project(prec_wettest_month, dem)
prec_driest_month <- project(prec_driest_month, dem)

# Check again with *copmareGeom* function
compareGeom(dem, temp_mean, temp_range, prec_annual, prec_wettest_month, prec_driest_month, tree_cover, canopy_height)

sapply(list(dem=dem, temp_mean=temp_mean, temp_range=temp_range, prec_annual=prec_annual, prec_wettest_month=prec_wettest_month, prec_driest_month=prec_driest_month, tree_cover=tree_cover, canopy_height=canopy_height),
       FUN=function(x){c(origin=origin(x), res=res(x), ext=ext(x)[1:4])})


## Stacking the layers
predictors <- c(dem, temp_mean, temp_range, prec_annual, prec_wettest_month, prec_driest_month, tree_cover, canopy_height)
names(predictors) <- c('dem', 'temp_mean', 'temp_range', 'prec_annual', 'prec_wettest_month', 'prec_driest_month', 'tree_cover', 'canopy_height')
predictors


writeRaster(temp_mean, 'temp_mean.tif')
writeRaster(temp_range, 'temp_range.tif')
writeRaster(prec_annual, 'precipitation_annual.tif')
writeRaster(prec_wettest_month, 'precipitation_wettest_month.tif')
writeRaster(prec_driest_month, 'precipitatioin_driest_month.tif')



## Read in rasters
## Read in projected files
# Mean annual air temperature (bio1)
temp_mean <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\temp_mean.tif')
# Annual range of air temperature (bio7)
temp_range <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\temp_range.tif')
# Annual Precipitation amount (bio12)
prec_annual <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\precipitation_annual.tif')
# Precipitation amount of the wettest month (bio13)
prec_wettest_month <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\precipitation_wettest_month.tif')
# Precipitation amount of the driest month (bio14)
prec_driest_month <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\precipitatioin_driest_month.tif')
# DEM
dem <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\DEM_aea.tif')
# Tree cover
tree_cover <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\tree_cover_2000_aea.tif')
# Canopy height
canopy_height <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\canopy_height_aea.tif')

# Tree cover
tree_cover <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\tree_cover_2000_aea.tif')
# Canopy height
canopy_height <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\canopy_height_aea.tif')

#convert to Spatraster
slope <- as(slope, "SpatRaster")
aspect <- as(aspect, "SpatRaster")
pop_dens <- as(pop_dens, "SpatRaster")
dem <- as(dem, "SpatRaster")


#water bodies
install.packages("sf")
library(sf)

lakes <- read_sf("C:/Users/justi/OneDrive/Studium/Umweltnaturwissenschaften/Landscape Modelling/Condor/download polybox/sa_lakes.shp")

rivers <- read_sf("C:/Users/justi/OneDrive/Studium/Umweltnaturwissenschaften/Landscape Modelling/Condor/download polybox/HydroRIVERS_v10_sa.shp")

(water <- st_union(lakes[, 'geometry'], rivers[, 'geometry'], col='blue')) #merge lakes & rivers
plot(st_geometry(water), col='lightblue')
water_buffer <- st_buffer(water, 500) #create water buffer
water_buffer_r <- rasterize(water_buffer, dem) #rasterize
writeRaster(water_buffer_r, 'Raster/water_buffer.tif')
```

Extracting environmental conditions for presences and absences:

    df_pres_aa <- cbind(extract(environmental_predictors,crd_aa_thin), crd_aa_thin)

Code for reprojection:

dem_reprojected \<- terra::project(x = dem, y = "ESRI:102033", method = "near")

```{r}
####

################################
model_matrix_condor
model_matrix_puma
model_matrix_bear

summary(model_matrix_condor)
summary(model_matrix_puma)
summary(model_matrix_bear)
################################

### Creating a model matrix
# combining observation data of the species with the environmental data at the sampling points
# Matrix must consist the coordinates of the sampling points, presences and absences, and the local environmental conditions

model_matrix <- condor
names(model_matrix)[4] <- 'condor'
model_matrix$puma <- puma$Presence[match(model_matrix$X, puma$X)]
model_matrix$bear <- bear$Presence[match(model_matrix$X, bear$X)]
summary(model_matrix)

# Add environmental values and remove NAs
model_matrix <- cbind(model_matrix, extract(predictors, model_matrix[, c('x', 'y')], ID=F))
summary(model_matrix)
model_matrix <- model_matrix[complete.cases(model_matrix),]
summary(model_matrix)

# Save matrix
# write.Raster(model_matrix, 'model_matrix.tif')

# Check number of presences and absences for each species (here only presences)
table(model_matrix$condor)
table(model_matrix$puma)
table(model_matrix$bear)


## Plot the data (does not work here but works in my own RStudio)
# Condor
plot(dem, axes = FALSE, mar = c(.5,.5,1,.5), col=viridis(1e3))
points(model_matrix$x[model_matrix$condor==1], 
       model_matrix$y[model_matrix$condor==1], #Presences
       col = 'yellow', pch = 16, cex = 0.75)
points(model_matrix$x[model_matrix$condor==0], 
       model_matrix$y[model_matrix$condor==0], #Absences
       col = 'red', pch = 16, cex = 0.75)

# Puma
plot(dem, axes = FALSE, mar = c(.5,.5,1,.5), col=viridis(1e3))
points(model_matrix$x[model_matrix$puma==1], 
       model_matrix$y[model_matrix$puma==1], #Presences
       col = 'yellow', pch = 16, cex = 0.75)
points(model_matrix$x[model_matrix$puma==0], 
       model_matrix$y[model_matrix$puma==0], #Absences
       col = 'red', pch = 16, cex = 0.75)

# Bear
plot(dem, axes = FALSE, mar = c(.5,.5,1,.5), col=viridis(1e3))
points(model_matrix$x[model_matrix$bear==1], 
       model_matrix$y[model_matrix$bear==1], #Presences
       col = 'yellow', pch = 16, cex = 0.75)
points(model_matrix$x[model_matrix$bear==0], 
       model_matrix$y[model_matrix$bear==0], #Absences
       col = 'red', pch = 16, cex = 0.75)
###### ---> Points are the same in all plots


#### Correlation among predictors with model_matrix
### From the lab:
# Subset the model_matrix for environmental predictors
vls <- model_matrix_condor[, 6:ncol(model_matrix_condor)]

# Calcualte all pairwise Pearson correlation coefficients
corma <- as.matrix(cor(vls))

# Prepare the plot
par(mfrow = c(1,1), oma = c(0,7.5,7,0), mar = c(0,0,0,0), ps = 8, cex = 1, xpd = NA)

plot(1, 1, xlim = c(0, ncol(vls)-.5), ylim = c(0.5, ncol(vls)), 
     xaxs = "i", yaxs = "i", type = "n", xaxt = "n", yaxt = "n", bty = "n", 
     ylab = "", xlab = "")

# Loop over the upper left half of the correlation matrix and plot the values
for(i in 1:(ncol(vls)-1)){
  
  if(i<ncol(vls)){
    text(i-.5, ncol(vls)+.3, colnames(vls)[i], pos=2,offset=0,srt=-90) #x-axis labels
  }
  
  for(j in (i+1):ncol(vls)){
    # Define color code: green = OK, orange = problematic, red = big problem
    cl <- ifelse(abs(corma[i,j]) < .7, "green", 
                 ifelse(abs(corma[i,j]) < .9, "orange", "red"))
    points(i-.5, j-.5, cex = 5, pch = 16, col = cl)
    # Add Pearson correlation coefficients
    text(i-.5, j-.5, round(corma[i,j], digits = 2), cex = .9) 
    
    if(i==1){
      text(i-.5, j-.5, colnames(vls)[j], pos = 2, offset = 2) # y-axis labels
    }
  }
}
par(xpd = NA)
#(does not work here but works in my own RStudio)
#Do for each model_matrix (condor, puma, bear)?



```




## Chunk 5: Fitting first Models (Manuel)

The following chunk does the following:

-   For each of the three species, one GLM and one RF model is fitted based on the random set of pseudo-absences at a resolution of 1x1 km. Pseudo-absences that fall within cells that contain presences are discarded.

-   The models are optimized by excluding predictors with less explanatory power (stepwise AIC optimization).

-   Response curves are generated.

```{r}
## Reading in presence data
condor <- read.csv("Occurrence data/condor_presences_1km.csv")
puma <- read.csv("Occurrence data/puma_presences_1km.csv")
bear <- read.csv("Occurrence data/bear_presences_1km.csv")

## Reading in absence data
absences_random <- read.csv("Pseudo absences/pseudo absences random.csv")

## Combining the two dataframes and removing pseudo absences
# Condor
pre_crd_condor <- data.frame(x = condor$x, y = condor$y, Presence = condor$Presence)
abs_crd <- data.frame(x = absences_random$x, y = absences_random$y, Presence = absences_random$Presence)
abs <- rbind(abs_crd, pre_crd_condor) # absences first, as we will extract those
library(misty)
duplicates <- as.numeric(rownames(df.unique(df.duplicated(abs, x, y))))
condor <- abs[-duplicates,]

# Puma
pre_crd_puma <- data.frame(x = puma$x, y = puma$y, Presence = puma$Presence)
abs_crd <- data.frame(x = absences_random$x, y = absences_random$y, Presence = absences_random$Presence)
abs <- rbind(abs_crd, pre_crd_puma) # absences first, as we will extract those
library(misty)
duplicates <- as.numeric(rownames(df.unique(df.duplicated(abs, x, y))))
puma <- abs[-duplicates,]

# Bear
pre_crd_bear <- data.frame(x = bear$x, y = bear$y, Presence = bear$Presence)
abs_crd <- data.frame(x = absences_random$x, y = absences_random$y, Presence = absences_random$Presence)
abs <- rbind(abs_crd, pre_crd_bear) # absences first, as we will extract those
library(misty)
duplicates <- as.numeric(rownames(df.unique(df.duplicated(abs, x, y))))
bear <- abs[-duplicates,]

# Reading in environmental predictors (adjust path)
predictors <- rast("predictors.tif")

# Extracting environmental predictors (element predictors)
model_matrix_condor <- cbind(condor, extract(predictors, condor[, c("x", "y")], ID = F))
model_matrix_puma <- cbind(puma, extract(predictors, puma[, c("x", "y")], ID = F))
model_matrix_bear <- cbind(bear, extract(predictors, bear[, c("x", "y")], ID = F))

# Removing NAs
model_matrix_condor <- model_matrix_condor[complete.cases(model_matrix_condor),]
summary(model_matrix_condor)
model_matrix_puma <- model_matrix_puma[complete.cases(model_matrix_puma),]
summary(model_matrix_puma)
model_matrix_bear <- model_matrix_bear[complete.cases(model_matrix_bear),]
summary(model_matrix_bear)


####################################################################################
# GLM
####################################################################################

# Define formula
form_base_condor <- Presence ~ predictorname1 + I(predictorname1^2) + ...
form_base_puma <- Presence ~ predictorname1 + I(predictorname1^2) + ...
form_base_bear <- Presence ~ predictorname1 + I(predictorname1^2) + ...

# Fit full model
glm_condor_full <- glm(form_base_condor, data = model_matrix_condor, family = 'binomial')
summary(glm_condor_full)
glm_puma_full <- glm(form_base_puma, data = model_matrix_puma, family = 'binomial')
summary(glm_puma_full)
glm_bear_full <- glm(form_base_bear, data = model_matrix_bear, family = 'binomial')
summary(glm_bear_full)

## Stepwise model optimization
glm_condor_step <- step(glm_condor_full, directions = 'both', trace = 0)
summary(glm_condor_step)
glm_puma_step <- step(glm_puma_full, directions = 'both', trace = 0)
summary(glm_puma_step)
glm_bear_step <- step(glm_bear_full, directions = 'both', trace = 0)
summary(glm_bear_step)

## Check effect of simpler model on adjD2
ecospat.adj.D2.glm(glm_condor_full)
ecospat.adj.D2.glm(glm_condor_step)
ecospat.adj.D2.glm(glm_puma_full)
ecospat.adj.D2.glm(glm_puma_step)
ecospat.adj.D2.glm(glm_bear_full)
ecospat.adj.D2.glm(glm_bear_step)

# Generating response curves #################################################

env <- values(predictors)

# Extract minima, maxima, and medians
env_stats <-apply(env, 2, quantile, probs = c(0,.5,1), na.rm = TRUE) 
round(env_stats, digits = 2)

# Identifying the predictors in the step GLM
all_preds # Get all predictor names

### Condor
# Check for each whether it is in the model equation
preds_glm <- vector()
for(i in all_preds){
  preds_glm[i] <- grepl(i, as.character(glm_condor_step$formula)[3]) 
}

# Get subset of predictors that are contained in the model equation
used_preds <- all_preds[preds_glm]

# Prepare raw prediction data set (200 replicates of medians for all predictors)
predat_raw <- as.data.frame(env_stats[rep(2,200), all_preds])

# Prepare plotting window
par(mfrow=c(3,3), cex = 1, mar = c(4,4,1,1))

# Loop over used predictors
for(i in used_preds){
  
  # Create data set for prediction
  predat_i <- predat_raw
  
  # Create sequence for the variable of interest (min to max in 200 steps)
  predat_i[,i] <- seq(env_stats[1,i], env_stats[3,i], length.out = 200)
  
  # Create plot frame for response curves of predictor i
  plot(1,1,ylim=c(0,1),xlim=env_stats[c(1,3),i], type = "n", 
       ylab = "Occurrence probability", xlab = i)
  
  # Predict nutcracker
  prd_condor_i <- predict(glm_condor_step, newdata = predat_i, type = "response", se.fit = TRUE)
  
  # Add polygon for standard error
  polygon(c(predat_i[,i], rev(predat_i[,i]), predat_i[1,i]),
          c(prd_condor_i$fit + prd_condor_i$se.fit, rev(prd_condor_i$fit - prd_condor_i$se.fit), 
            prd_condor_i$fit[1] + prd_condor_i$se.fit[1]),
          col = "#e5f5f9", border = FALSE)
  
  # Add line for prediction
  lines(predat_i[,i], prd_condor_i$fit, col = "#2ca25f")
  
  # Add observations
  points(model_matrix_condor[,i], model_matrix_condor$Presence, pch = "I", col = "#00000050")## this line has to be revisited
  
  # Redraw panel box
  box()
}

### Puma
# Check for each whether it is in the model equation
preds_glm <- vector()
for(i in all_preds){
  preds_glm[i] <- grepl(i, as.character(glm_puma_step$formula)[3]) 
}

# Get subset of predictors that are contained in the model equation
used_preds <- all_preds[preds_glm]

# Prepare raw prediction data set (200 replicates of medians for all predictors)
predat_raw <- as.data.frame(env_stats[rep(2,200), all_preds])

# Prepare plotting window
par(mfrow=c(3,3), cex = 1, mar = c(4,4,1,1))

# Loop over used predictors
for(i in used_preds){
  
  # Create data set for prediction
  predat_i <- predat_raw
  
  # Create sequence for the variable of interest (min to max in 200 steps)
  predat_i[,i] <- seq(env_stats[1,i], env_stats[3,i], length.out = 200)
  
  # Create plot frame for response curves of predictor i
  plot(1,1,ylim=c(0,1),xlim=env_stats[c(1,3),i], type = "n", 
       ylab = "Occurrence probability", xlab = i)
  
  # Predict nutcracker
  prd_puma_i <- predict(glm_puma_step, newdata = predat_i, type = "response", se.fit = TRUE)
  
  # Add polygon for standard error
  polygon(c(predat_i[,i], rev(predat_i[,i]), predat_i[1,i]),
          c(prd_puma_i$fit + prd_puma_i$se.fit, rev(prd_puma_i$fit - prd_puma_i$se.fit), 
            prd_puma_i$fit[1] + prd_puma_i$se.fit[1]),
          col = "#e5f5f9", border = FALSE)
  
  # Add line for prediction
  lines(predat_i[,i], prd_puma_i$fit, col = "#2ca25f")
  
  # Add observations
  points(model_matrix_puma[,i], model_matrix_puma$Presence, pch = "I", col = "#00000050")## this line has to be revisited
  
  # Redraw panel box
  box()
}

### Bear
# Check for each whether it is in the model equation
preds_glm <- vector()
for(i in all_preds){
  preds_glm[i] <- grepl(i, as.character(glm_bear_step$formula)[3]) 
}

# Get subset of predictors that are contained in the model equation
used_preds <- all_preds[preds_glm]

# Prepare raw prediction data set (200 replicates of medians for all predictors)
predat_raw <- as.data.frame(env_stats[rep(2,200), all_preds])

# Prepare plotting window
par(mfrow=c(3,3), cex = 1, mar = c(4,4,1,1))

# Loop over used predictors
for(i in used_preds){
  
  # Create data set for prediction
  predat_i <- predat_raw
  
  # Create sequence for the variable of interest (min to max in 200 steps)
  predat_i[,i] <- seq(env_stats[1,i], env_stats[3,i], length.out = 200)
  
  # Create plot frame for response curves of predictor i
  plot(1,1,ylim=c(0,1),xlim=env_stats[c(1,3),i], type = "n", 
       ylab = "Occurrence probability", xlab = i)
  
  # Predict nutcracker
  prd_bear_i <- predict(glm_bear_step, newdata = predat_i, type = "response", se.fit = TRUE)
  
  # Add polygon for standard error
  polygon(c(predat_i[,i], rev(predat_i[,i]), predat_i[1,i]),
          c(prd_bear_i$fit + prd_bear_i$se.fit, rev(prd_bear_i$fit - prd_bear_i$se.fit), 
            prd_bear_i$fit[1] + prd_bear_i$se.fit[1]),
          col = "#e5f5f9", border = FALSE)
  
  # Add line for prediction
  lines(predat_i[,i], prd_bear_i$fit, col = "#2ca25f")
  
  # Add observations
  points(model_matrix_bear[,i], model_matrix_bear$Presence, pch = "I", col = "#00000050")## this line has to be revisited
  
  # Redraw panel box
  box()
}

####################################################################################
# Random Forest
####################################################################################

library(ranger)
form_rf = Presence ~ predictor1 + ...
rf_condor <- ranger(form_rf, data = model_matrix_condor, num.trees = 500, probability = TRUE)
rf_condor
rf_puma <- ranger(form_rf, data = model_matrix_puma, num.trees = 500, probability = TRUE)
rf_puma
rf_bear <- ranger(form_rf, data = model_matrix_bear, num.trees = 500, probability = TRUE)
rf_bear

# Generating response curves ######################################################

### Condor
# Prepare plotting window
par(mfrow=c(3,3), cex = 1, mar = c(4,4,1,1))

# Loop over used predictors
for(i in used_preds){
  
  # Create data set for prediction.
  predat_i <- predat_raw
  
  # Create sequence for the variable of interest (min to max in 200 steps)
  predat_i[,i] <- seq(env_stats[1,i], env_stats[3,i], length.out = 200)
  
  # Create plot frame for response curves of predictor i
  plot(1,1, ylim = c(0,1), xlim = env_stats[c(1,3),i], type = "n", 
       ylab = "Occurrence probability", xlab = i)
  
  # Predict condor
  prd_i <- predict(rf_condor, data = predat_i, type = "response", predict.all = TRUE)
  
  # Do a bootstrap across the all 500 trees to obtain empirical confidence intervals
  tre_pr <- prd_i$predictions[,2,]  # Tree by tree predictions 
  boot_reps <- list()
  for(j in 1:100){ # Loop over bootstrap replicates
    boot_i <- matrix(NA, ncol = ncol(tre_pr), nrow = nrow(tre_pr))
    for(k in 1:nrow(tre_pr)){ # Loop over rows (env. gradient)
      # Resample 500 trees with replacement
      smp_k <- sample(1:ncol(tre_pr), replace = T)
      boot_i[k,] = tre_pr[k,smp_k] # Generate an ensemble of resampled trees
    }
    boot_reps[[j]] <- rowMeans(boot_i) # Calculate probabilities from the ensemble
  }
  mat_boot <- do.call("cbind", boot_reps)
  # Obtain median and confidence intervals from the resampled probabilities
  confi_i <- apply(mat_boot, 1, quantile, probs = c(0.025,0.5,0.975)) 
  
  # Add polygon for empirical confidence interval
  polygon(c(predat_i[,i], rev(predat_i[,i]), predat_i[1,i]),
          c(confi_i[1,], rev(confi_i[3,]), confi_i[1,1]),
          col = "#e5f5f9", border = FALSE)
  
  # Add line for prediction
  lines(predat_i[,i], rowMeans(tre_pr), col = "#2ca25f")
  
  # Add observations
  points(model_matrix_condor[,i], model_matrix_condor$Presence, pch = "I", col = "#00000050")## this line has to be revisited
  
  # Redraw panel box
  box()
}

### Puma
# Prepare plotting window
par(mfrow=c(3,3), cex = 1, mar = c(4,4,1,1))

# Loop over used predictors
for(i in used_preds){
  
  # Create data set for prediction.
  predat_i <- predat_raw
  
  # Create sequence for the variable of interest (min to max in 200 steps)
  predat_i[,i] <- seq(env_stats[1,i], env_stats[3,i], length.out = 200)
  
  # Create plot frame for response curves of predictor i
  plot(1,1, ylim = c(0,1), xlim = env_stats[c(1,3),i], type = "n", 
       ylab = "Occurrence probability", xlab = i)
  
  # Predict puma
  prd_i <- predict(rf_puma, data = predat_i, type = "response", predict.all = TRUE)
  
  # Do a bootstrap across the all 500 trees to obtain empirical confidence intervals
  tre_pr <- prd_i$predictions[,2,]  # Tree by tree predictions 
  boot_reps <- list()
  for(j in 1:100){ # Loop over bootstrap replicates
    boot_i <- matrix(NA, ncol = ncol(tre_pr), nrow = nrow(tre_pr))
    for(k in 1:nrow(tre_pr)){ # Loop over rows (env. gradient)
      # Resample 500 trees with replacement
      smp_k <- sample(1:ncol(tre_pr), replace = T)
      boot_i[k,] = tre_pr[k,smp_k] # Generate an ensemble of resampled trees
    }
    boot_reps[[j]] <- rowMeans(boot_i) # Calculate probabilities from the ensemble
  }
  mat_boot <- do.call("cbind", boot_reps)
  # Obtain median and confidence intervals from the resampled probabilities
  confi_i <- apply(mat_boot, 1, quantile, probs = c(0.025,0.5,0.975)) 
  
  # Add polygon for empirical confidence interval
  polygon(c(predat_i[,i], rev(predat_i[,i]), predat_i[1,i]),
          c(confi_i[1,], rev(confi_i[3,]), confi_i[1,1]),
          col = "#e5f5f9", border = FALSE)
  
  # Add line for prediction
  lines(predat_i[,i], rowMeans(tre_pr), col = "#2ca25f")
  
  # Add observations
  points(model_matrix_puma[,i], model_matrix_puma$Presence, pch = "I", col = "#00000050")## this line has to be revisited
  
  # Redraw panel box
  box()
}

### Bear
# Prepare plotting window
par(mfrow=c(3,3), cex = 1, mar = c(4,4,1,1))

# Loop over used predictors
for(i in used_preds){
  
  # Create data set for prediction.
  predat_i <- predat_raw
  
  # Create sequence for the variable of interest (min to max in 200 steps)
  predat_i[,i] <- seq(env_stats[1,i], env_stats[3,i], length.out = 200)
  
  # Create plot frame for response curves of predictor i
  plot(1,1, ylim = c(0,1), xlim = env_stats[c(1,3),i], type = "n", 
       ylab = "Occurrence probability", xlab = i)
  
  # Predict bear
  prd_i <- predict(rf_bear, data = predat_i, type = "response", predict.all = TRUE)
  
  # Do a bootstrap across the all 500 trees to obtain empirical confidence intervals
  tre_pr <- prd_i$predictions[,2,]  # Tree by tree predictions 
  boot_reps <- list()
  for(j in 1:100){ # Loop over bootstrap replicates
    boot_i <- matrix(NA, ncol = ncol(tre_pr), nrow = nrow(tre_pr))
    for(k in 1:nrow(tre_pr)){ # Loop over rows (env. gradient)
      # Resample 500 trees with replacement
      smp_k <- sample(1:ncol(tre_pr), replace = T)
      boot_i[k,] = tre_pr[k,smp_k] # Generate an ensemble of resampled trees
    }
    boot_reps[[j]] <- rowMeans(boot_i) # Calculate probabilities from the ensemble
  }
  mat_boot <- do.call("cbind", boot_reps)
  # Obtain median and confidence intervals from the resampled probabilities
  confi_i <- apply(mat_boot, 1, quantile, probs = c(0.025,0.5,0.975)) 
  
  # Add polygon for empirical confidence interval
  polygon(c(predat_i[,i], rev(predat_i[,i]), predat_i[1,i]),
          c(confi_i[1,], rev(confi_i[3,]), confi_i[1,1]),
          col = "#e5f5f9", border = FALSE)
  
  # Add line for prediction
  lines(predat_i[,i], rowMeans(tre_pr), col = "#2ca25f")
  
  # Add observations
  points(model_matrix_bear[,i], model_matrix_bear$Presence, pch = "I", col = "#00000050")## this line has to be revisited
  
  # Redraw panel box
  box()
}
```
