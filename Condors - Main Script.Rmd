---
title: "Landscape Modelling Project - Condors - Main Script"
author: "Justine DeGroote, Tanja Falasca, Manuel Weber"
date: '2023-03-31'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Landscape Modelling Project - Condors

## Main Script

##### Students: Justine DeGroote, Tanja Falasca, Manuel Weber

##### Supervisor: Monika K. Goralczyk

![Andean Condor (Pixabay)](condor.jpg)

## Organization

Weekly meetings on Tuesdays at 4:15 PM on the green floor in CHN or in F77 (no meeting on the 11th of April).

Summarize and email questions to Monika before the meetings.

| Date        | Progress Milestone                                                                          | Suggested Focus                                                     |
|------------------|-------------------------------|-----------------------|
| 03.04-07.04 | Species occurrence data downloaded and cleaned                                              | Data retrieval and cleaning                                         |
| 10.04-14.05 | Environmental data prepared for model fitting                                               | Environmental exploration                                           |
| 17.04-21.04 | 1.Initial set of predictors selected 2.Created 1st set of PAs (e.g.: random)                | Environmental predictors ; Pseudoabsence creation (PAs)             |
| 24.04-28.04 | Buffered, target-group, environmentally stratified PAs                                      | Pseudoabsence creation (PAs)                                        |
| 01.05-05.05 | Models fitted and evaluated (at least 2 algorithms)                                         | Model fitting & evaluation                                          |
| 08.05-12.05 | Evidence of exploring different model parameters, selected ones with best fit and accuracy. | Generated predictor response curves Model fitting & model selection |
| 15.05-19.05 | Creation of ranges (polygons for mammals)                                                   | Alternative approach to SDM                                         |
| 22.05-26.05 | Overlap of distributions and protected areas                                                | Implications for conservation                                       |
| 29.05       | Presentation                                                                                |                                                                     |

: Preliminary schedule

## Week 1: Species Occurrence Data Download and Cleaning

Examine records from GBIF, remove erroneous entries and ones without geolocation, and duplicate records. Remove low-quality records (e.g.: based on spatial and temporal accuracy) and outliers (compare with range maps).

If there is time, familiarize yourself with environmental data (crop to the study extent, broadly), download bioclimatic data from Chelsea ([https://chelsa-climate.org/bioclim/)](https://chelsa-climate.org/bioclim/) explore additional data (topography, rivers and lakes shapefiles (these will need to be rasterized and distance calculated), land cover, degree of openness etc.)

```{r}
# Data acquisition
# Load the rgbif R package
# library(rgbif)
# 
# # Download occurrences for the three species
# ## Vultur gryphus
# gbi_vg <- occ_search(scientificName = "Vultur gryphus", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_vg$data, "gbi_vg.csv")
# 
# ## Puma concolor
# gbi_pc <- occ_search(scientificName = "Puma concolor", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_pc$data, "gbi_pc.csv")
# 
# ## Tremarctos ornatus
# gbi_to <- occ_search(scientificName = "Tremarctos ornatus", 
#                      continent = "south_america", 
#                      hasCoordinate = TRUE, # Only observations with coordinates
#                      eventDate = "2000,2023", # Observations between 1980 and 2023
#                      hasGeospatialIssue = FALSE, # No geospatial issues 
#                      limit = 100000) # Maxiumum number of records
# 
# write.csv(gbi_to$data, "gbi_to.csv")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 6/4; 13:22)
############################################################################################


<<<<<<< HEAD
  # Extract essential columns
  gbi_vg_crd <- gbi_vg_crd$data[,c("decimalLongitude", "decimalLatitude", 
                             "coordinateUncertaintyInMeters", "occurrenceStatus")]
  gbi_vg_crd$occurrenceStatus <- as.factor(gbi_vg_crd$occurrenceStatus)


# Inspect output
dim(gbi_vg_crd)
summary(gbi_vg_crd)

#We have 40'039 observations, of which 37'902 have no data about the coordinate precision.

# Fraction of imprecise observations 
tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=1000)
round(as.numeric(tb_vg[1]/tb_vg), digits = 2)

sum(gbi_vg_crd$coordinateUncertaintyInMeters < 1000)

# Create SpatVector
suppressPackageStartupMessages(library(dismo))
ve_vg = vect(gbi_vg_ss,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:4326")
ve_vg = vect(gbi_vg_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:4326")

# Plot the data
plot(shape_ch, main = expression(paste(italic('Vultur gryphus'))), 
     axes = FALSE, mar = c(.5,.5,1,.5))
points(ve_vg, col = '#004D4050', pch = 16, cex = .8)
```

#### SDM resources

Zurell, D. et al. (2020) "A standard protocol for reporting species distribution models," Ecography, 43(9), pp. 1261--1277. Available at: <https://doi.org/10.1111/ecog.04960>

ENM2020 course (table 1 contains description of tutorials) <https://journals.ku.edu/jbi/article/view/15016/15152>

For cleaning occurrence data (from the ENM 2020 course)

-   'Tools for biodiversity data cleaning' -- R packages for data cleaning

-   'Occurrence data cleaning I (simple consistency checks)' -- main manual checks

Some information about habitat and diet of the 3 focal species <https://avianreport.com/andean-condor-range-and-habitat/> <https://nationalzoo.si.edu/animals/andean-bear#:~:text=Andean%20bears%20live%20in%20a,forest%20to%20thorny%20dry%20forest.> <https://animalia.bio/south-american-cougar>

```{r}
# Cleaning, projecting and visualizing data
# Loading packages
lib_vect <- c("terra", "ranger", "ecospat", "dismo", "rworldmap")
sapply(lib_vect,require,character.only=TRUE)

# Read in data
condor <- read.csv("Occurence data/gbi_vg.csv")
puma <- read.csv("Occurence data/gbi_pc.csv")
bear <- read.csv("Occurence data/gbi_to.csv")


# Extract essential columns
gbi_vg_crd <- condor[,c("decimalLongitude", "decimalLatitude", 
                             "coordinateUncertaintyInMeters", "occurrenceStatus")]
gbi_vg_crd$occurrenceStatus <- as.factor(gbi_vg_crd$occurrenceStatus)
gbi_pc_crd <- puma[,c("decimalLongitude", "decimalLatitude", 
                             "coordinateUncertaintyInMeters", "occurrenceStatus")]
gbi_pc_crd$occurrenceStatus <- as.factor(gbi_pc_crd$occurrenceStatus)
gbi_to_crd <- bear[,c("decimalLongitude", "decimalLatitude", 
                             "coordinateUncertaintyInMeters", "occurrenceStatus")]
gbi_to_crd$occurrenceStatus <- as.factor(gbi_to_crd$occurrenceStatus)


# Inspect output
dim(gbi_vg_crd)
summary(gbi_vg_crd)
dim(gbi_pc_crd)
summary(gbi_pc_crd)
dim(gbi_to_crd)
summary(gbi_to_crd)


# Create SpatVector
## We choose an area-conservative projection, namely Albers Equal Area, EPSG:9822
## See also https://epsg.io/9822-method
ve_vg = vect(gbi_vg_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:9822")
ve_pc = vect(gbi_pc_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:9822")
ve_to = vect(gbi_to_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:9822")

# Visualize
data(coastsCoarse)
{par(mfrow = c(1,3))
plot(ve_vg, main = expression(paste(italic('Vultur gryphus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
plot(coastsCoarse, add = T, col = "grey")
plot(ve_pc, main = expression(paste(italic('Puma concolor'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
plot(coastsCoarse, add = T, col = "grey")
plot(ve_to, main = expression(paste(italic('Tremarctos ornatus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
plot(coastsCoarse, add = T, col = "grey")}

## Next steps (Manuel): Mask water bodies, possibly crop the data to northern South America, acquire environmental data from worldclim, possibly disaggregate occurence data based on resolution of environmental rasters.
```

## Week 2: Data Preparation

Goal: selecting, cropping, reprojecting, and aligning rasters.

Rasters to consider:

-   Bioclimatic (obtain from CHELSA <https://chelsa-climate.org/bioclim/>)

-   Tree cover

-   Tree height

-   DEM (later calculate aspect and slope in R

-   Population density

-   Distance to water

-   Landscape openness

Decide on trade-off between grain size and extent

Mask water bodies

Maybe limit scope to northern south America

Splitting data into months, fitting months separately (is one habitat more important for nesting?)

Environmental or spatial stratification of pseudo-absences

We start with random pseudo-absences

We choose to use an area-conservative projection: Albers equal area conic projection (epsg.io)

```{r}
# Generating slope and aspect from DEM:
# library(terra)
# (dem <- rast("Rasters/DEM.tif"))
# slope <- terrain(x = dem, v = "slope")
# aspect <- terrain(x = dem, v = "aspect")
# 
# writeRaster(slope, "Rasters/slope.tif")
# writeRaster(aspect, "Rasters/aspect.tif")

############################################################################################
#The code above has been run, the files are stored in the repository, please don't run again but access the files directly (Manuel 13/4; 17:47)
############################################################################################

# I uploaded the following rasters to the repository:
# DEM, tree cover, canopy height, slope, aspect
# Still missing: Rivers, population density, bioclimatic variables

#insert Rasters
pop_dens <- raster("C:/Users/justi/OneDrive/Studium/Umweltnaturwissenschaften/Landscape Modelling/Condor/download polybox/gpw_v4_population_density_rev11_2020_30_sec.tif")

#convert to Spatraster
slope <- as(slope, "SpatRaster")
aspect <- as(aspect, "SpatRaster")
pop_dens <- as(pop_dens, "SpatRaster")
dem <- as(dem, "SpatRaster")

pop_dens <- project(pop_dens, slope) #same projection as slope
plot(pop_dens, axes = F, col = rainbow(73))

compareGeom(slope, pop_dens)

### Climatic predictors
## Temperature
# Mean annual air temperature
bio1 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio1_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')
#Annual range of air temperature
bio7 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio7_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')

## Precipitation
# Annual Precipitation amount
bio12 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio12_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')
# Precipitation amount of the wettest month
bio13 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio13_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')
# Precipitation amount of the driest month
bio14 <- rast('C:\\Users\\tanja\\OneDrive\\Dokumente\\UZH FS_23\\Landscape Modelling\\Project\\CHELSA_bio14_2011-2040_gfdl-esm4_ssp126_V.2.1.tif')

## Tree cover
tree_cover <- rast("C:\Users\tanja\OneDrive\Dokumente\UZH FS_23\Landscape Modelling\Project\API_AG.LND.FRST.ZS_DS2_en_csv_v2_5358376\API_AG.LND.FRST.ZS_DS2_en_csv_v2_5358376.csv")

## Create one predictor layer
# Check if the resolution, origin and coordinate reference system match among the different layers with the *compareGeom* function
compareGeom(dem, bio1, bio7, bio12, bio13, bio14)

# Reproject the *bio* raster so they have the same spatial properties as the *dem* raster
bio1 <- project(bio1, dem)
bio7 <- project(bio7, dem)
bio12 <- project(bio12, dem)
bio13 <- project(bio13, dem)
bio14 <- project(bio14, dem)

# Check again with *copmareGeom* function
compareGeom(dem, bio1, bio7, bio12, bio13, bio14)

# Stacking of the layers
predictors <- c(dem, slope, aspect, pop_dens, bio1, bio7, bio12, bio13, bio14)
names(predictors) <- c('dem', 'bio1', 'bio7', 'bio12', 'bio13', 'bio14')predictors
predictors
```

```{r}
# Loading packages
lib_vect <- c("terra", "ranger", "ecospat", "dismo", "rworldmap", "dplyr", "sf")
sapply(lib_vect,require,character.only=TRUE)

rm(list = ls())

# Read in raw GBIF data
condor <- read.csv("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Occurence data/gbi_vg.csv")
puma <- read.csv("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Occurence data/gbi_pc.csv")
bear <- read.csv("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Occurence data/gbi_to.csv")

# Cleaning data
# # The sepecies infomration can be used to help identify duplicates. We can also identify other criteria to test for. 
# coordinate_flags<-clean_coordinates(x=gbifdataframe, lon="decimalLongitude", lat="decimalLatitude",
#                                     countries="countryCode",
#                                     species="scientificName",
#                                     tests= c("centroids","gbif","institutions","duplicates","seas","zeros"),
#                                     verbose=T)
# 
# # Add wheteher or not the points were flagged to the datafame
# gbifdataframe$flags<-as.factor(coordinate_flags$.summary)
# # Remove those points that are non-problematic
# gbifdataframe<-subset(gbifdataframe, gbifdataframe$flags=="TRUE")
# 
# # GBIF can sometimes include subspecies in their results. Let's make sure we only have species-level data.
# gbifdataframe<-subset(gbifdataframe, gbifdataframe$taxonRank=="SPECIES")
# # Taxonomy can often change, and multiple names may be accepted for a single species. Here we will choose 1 name- see the assignment for the species name to input here. 
# gbifdataframe<-subset(gbifdataframe, gbifdataframe$scientificName=="Ranunculus marginatus d'Urv.")



# Extract relevant columns
gbi_vg_crd <- condor[,c("decimalLongitude", "decimalLatitude", 
                        "coordinateUncertaintyInMeters", "occurrenceStatus")]
gbi_vg_crd$occurrenceStatus <- as.factor(gbi_vg_crd$occurrenceStatus)
gbi_pc_crd <- puma[,c("decimalLongitude", "decimalLatitude", 
                      "coordinateUncertaintyInMeters", "occurrenceStatus")]
gbi_pc_crd$occurrenceStatus <- as.factor(gbi_pc_crd$occurrenceStatus)
gbi_to_crd <- bear[,c("decimalLongitude", "decimalLatitude", 
                      "coordinateUncertaintyInMeters", "occurrenceStatus")]
gbi_to_crd$occurrenceStatus <- as.factor(gbi_to_crd$occurrenceStatus)

sum(is.na(gbi_vg_crd$decimalLongitude),
    is.na(gbi_vg_crd$decimalLatitude),
    is.na(gbi_pc_crd$decimalLongitude),
    is.na(gbi_pc_crd$decimalLatitude),
    is.na(gbi_to_crd$decimalLongitude),
    is.na(gbi_to_crd$decimalLatitude))

# Inspect output
dim(gbi_vg_crd) # 40039 observations for the condor
summary(gbi_vg_crd)
dim(gbi_pc_crd) # 3785 observations for the puma
summary(gbi_pc_crd)
dim(gbi_to_crd) # 7329 observations for the bear
summary(gbi_to_crd)

# Fraction of observations with no information on coordinate precision
sna_vg <- length(which(is.na(gbi_vg_crd$coordinateUncertaintyInMeters)))
sna_pc <- length(which(is.na(gbi_pc_crd$coordinateUncertaintyInMeters)))
sna_to <- length(which(is.na(gbi_to_crd$coordinateUncertaintyInMeters)))

round((sna_vg+sna_pc+sna_to)/(nrow(gbi_vg_crd)+nrow(gbi_pc_crd)+nrow(gbi_to_crd))*100, digits = 2)
## 91% of the data doesn't contains no information at all about coordinate precision
# Fraction of imprecise observations
tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=100000)
tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=100000)
tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=100000)

round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
## 1.5% of the observations are less precise than 100 km

tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=10000)
tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=10000)
tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=10000)

round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
## 63.4 % of the observations are less precise than 10 km

tb_vg <- table(gbi_vg_crd$coordinateUncertaintyInMeters<=1000)
tb_pc <- table(gbi_pc_crd$coordinateUncertaintyInMeters<=1000)
tb_to <- table(gbi_to_crd$coordinateUncertaintyInMeters<=1000)

round(as.numeric((tb_vg[1]+tb_pc[1]+tb_to[1])/(sum(tb_vg)+sum(tb_pc)+sum(tb_to))*100), digits = 2)
## 71.8% of the observations are less precise than 1 km

## Replacing NAs by value
gbi_vg_crd[is.na(gbi_vg_crd)] <- -999
gbi_pc_crd[is.na(gbi_pc_crd)] <- -999
gbi_to_crd[is.na(gbi_to_crd)] <- -999

# Crop data to northern South America
gbi_vg_crd <- filter(gbi_vg_crd, decimalLatitude >= mean(condor$decimalLatitude))
nrow(gbi_vg_crd)
## 15506 observations left
gbi_pc_crd <- filter(gbi_pc_crd, decimalLatitude >= mean(condor$decimalLatitude))
nrow(gbi_pc_crd)
## 2324 observations left
gbi_to_crd <- filter(gbi_to_crd, decimalLatitude >= mean(condor$decimalLatitude))
nrow(gbi_to_crd)
## 7329 observations left

# Visualizing the data
## Create spatial vectors with the projection of interest (epsg:9822)
### We choose an area-conservative projection, namely Albers Equal Area, EPSG:9822
### See also https://epsg.io/9822-method

## First we transform the decimal coordinates to northing/easting (projected coordinates)

coordinates(gbi_vg_crd) <- c("decimalLongitude", "decimalLatitude")
proj4string(gbi_vg_crd) <- CRS("+init=epsg:9822")
gbi_vg_crd <- spTransform(gbi_vg_crd, CRS("+init=epsg:3857"))
gbi_vg_crd <- as.data.frame(gbi_vg_crd)
str(gbi_vg_crd)

dem_reprojected <- project(dem, "epsg:9822") # it should be esri:102033

crs(esri:102033)


ve_vg = vect(gbi_vg_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:9822")
ve_pc = vect(gbi_pc_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:9822")
ve_to = vect(gbi_to_crd,geom=c("decimalLongitude","decimalLatitude"),
             crs="+init=epsg:9822")

data(coastsCoarse)
par(mfrow = c(1,3))
plot(ve_vg, main = expression(paste(italic('Vultur gryphus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
plot(coastsCoarse, add = T, col = "grey")
abline(h = mean(condor$decimalLatitude), col = "red") # add line to show the scope of the study
plot(ve_pc, main = expression(paste(italic('Puma concolor'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
plot(coastsCoarse, add = T, col = "grey")
abline(h = mean(condor$decimalLatitude), col = "red") # add line to show the scope of the study
plot(ve_to, main = expression(paste(italic('Tremarctos ornatus'))), col = '#004D4050', pch = 16, cex = .8, xlab = "Longitude", ylab = "Latitude")
plot(coastsCoarse, add = T, col = "grey")

# Mask water bodies - IMPOSSIBLE DUE TO INCOMPATIBLE RESOLUTION
# ## First, create the dataframes to wgs84 spatial vectors (the water bodies are in wgs84)
# ve_vg = vect(condorcropped,geom=c("decimalLongitude","decimalLatitude"))
# ve_pc = vect(pumacropped,geom=c("decimalLongitude","decimalLatitude"))
# ve_to = vect(bearcropped,geom=c("decimalLongitude","decimalLatitude"))
# 
# ## Read in the water body shapefiles
# lakes <- st_read("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Rasters/Hydro_LAKES/sa_lakes.shp")
# rivers <- st_read("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Rasters/Hydro_RIVERS/HydroRIVERS_v10_sa.shp")
# plot(st_geometry(lakes), col = "blue")
# plot(st_geometry(rivers), col = "lightblue", add = T)
# 
# (dem <- rast("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Rasters/DEM.tif")
# )

# Thinning the data (only one presence per raster cell, here we use the DEM as template)
trees <- rast("C:/0_Documents/10_ETH/12_Mon 13-16_CHN E42_Landscape Modelling of Biodiversity/Project/Condor-Landscape-Modelling/Rasters/tree_cover.tif")
plot(trees)

crs(dem) == crs(ve_vg)
dem_reprojected <- projectRaster(dem, crs='+proj=longlat +datum=WGS84')
crs(dem_reprojected) == crs(ve_vg)


## use max of stream importance for raster


dem_reprojected <- dem
crs(dem_reprojected) <- crs("epsg:9822")
dem_reprojected <- project(dem_reprojected, y = "epsg:9822")

?project
projectRaster(r, crs='+proj=longlat +datum=WGS84')
plot(dem_reprojected)

## Count how many points fall into each pixel of the environmental raster
sam_int_vg <- rasterize(x = ve_vg, y = dem_reprojected, fun = "length")

dem_reprojected
range(geom(ve_vg)[,3]) # x coordinates
range(geom(ve_vg)[,4]) # y coordinates

stat_spat_coordinates(ve_vg)
str(dem_reprojected)
sam_int_pc <- rasterize(ve_pc, dem_reprojected, fun = "length")
sam_int_to <- rasterize(ve_to, dem_reprojected, fun = "length")
?rasterize

## Plot histograms
par(mfrow = c(1,3))
hist(sam_int_vg, breaks = 100, xlab = "counts",
     main = expression(paste(italic('Vultur gryphus'))))
hist(sam_int_pc, breaks = 100, xlab = "counts",
     main = expression(paste(italic('Puma concolor'))))
hist(sam_int_to, breaks = 100, xlab = "counts",
     main = expression(paste(italic('Tremarctos ornatus'))))




clean_coordinates(x = gbifdata, lon = , lat = , countries = , species = , tests = c("centroids, "gbif, "insitutions"))


??coords2continent
install.packages("coords2country")
coords2continent(-23,8)

```
